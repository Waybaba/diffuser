# @package _global_

defaults:
  - /callbacks: default.yaml
  - /logger: wandb.yaml

global_cfg: # would be passed to actor, critic1, critic2, policy, env
  n_train_steps: 2000000
  n_steps_per_epoch: 10000 # so n_epochs = n_train_steps / n_steps_per_epoch
  horizon: 384

runner:
  _target_: src.runner.TrainControllerRunner
  _partial_: true



modelmodule:
  _target_: src.modelmodule.FillActModelModule
  _partial_: true
  net: 
    _target_: src.modelmodule.FillActWrapper
    _partial_: true
    net:
      - _target_: torch.nn.Linear
        _partial_: true
        out_features: 128
      - _target_: torch.nn.BatchNorm1d
        num_features: 128
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Dropout
        p: 0.2
      - _target_: torch.nn.Linear
        in_features: 128
        out_features: 64
      - _target_: torch.nn.BatchNorm1d
        num_features: 64
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Dropout
        p: 0.1
      - _target_: torch.nn.Linear
        _partial_: true
        in_features: 64
  metric_func:
    _target_: src.modelmodule.L1DistanceMetric
    _partial_: true
  loss_func:
    _target_: torch.nn.L1Loss
    _partial_: true
  optimizations:
    - param_target: all
      optimizer: 
        _target_: torch.optim.SGD
        _partial_: true
        lr: 0.01
        momentum: 0.9
        weight_decay: 0.0
      # lr_scheduler_config:
      #   scheduler:
      #     _target_: src.utils.utils.PolynomialLR
      #     # _target_: torch.optim.lr_scheduler.StepLR
      #     _partial_: true
      #     step_size: 1
      #     iter_warmup: 0
      #     iter_max: ${trainer.max_steps}
      #     power: 0.9
      #     min_lr: 1e-5
      #   interval: step
      #   frequency: 1
  optimization_first: ${modelmodule.optimizations.0} # for wandb log



datamodule: 
  _target_: diffuser.datasets.FillActDataModule
  _partial_: true
  env: "halfcheetah-medium-expert-v2"
  custom_ds_path: null
  batch_size: 1024
  pin_memory: false
  num_workers: ${oc.decode:${oc.env:NUM_WORKERS}}
  train_val_test_split: [0.9,0.05,0.05]
  renderer: 
    _target_: diffuser.utils.rendering.MuJoCoRenderer
    _partial_: true
    env: ${datamodule.env}


trainer:
    _target_: "pytorch_lightning.Trainer"
    _partial_: true
    default_root_dir: ${output_dir}
    # callbacks: ${callbacks}
    # logger: ${logger}
    min_epochs: null
    max_epochs: 10
    accelerator: "gpu"
    # devices: 1
    # move_metrics_to_cpu: true
    deterministic: false
    # max_steps: 40000.0
    log_every_n_steps: 100
    num_sanity_val_steps: 2
    check_val_every_n_epoch: 1


# common - for all tasks (task_name, tags, output_dir, device)
algorithm_name: "DefaultAlgName"
task_name: "RL_Diffuser"
tags: ["debug"]
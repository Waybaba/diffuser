# @package _global_

global_cfg: # would be passed to actor, critic1, critic2, policy, env
  n_train_steps: 2000000
  n_steps_per_epoch: 10000 # so n_epochs = n_train_steps / n_steps_per_epoch
  horizon: 64 # ! DEBUG to 64 for efficiency

runner:
  _target_: src.runner.TrainDiffuserRunner
  _partial_: true

dataset:
  _target_: diffuser.datasets.SequenceGPUDataset
  _partial_: true
  env: "maze2d-openlarge-v0"
  only_start_end_episode: true
  # custom_ds_path: ${paths.data_dir}/models/diffuser/maze2d-large-waybaba.hdf5
  custom_ds_path: null
  horizon: ${global_cfg.horizon}
  normalizer: "LimitsNormalizer"
  preprocess_fns: ["maze2d_set_terminals"] # why need this?
  # preprocess_fns: []
  # use_padding: false
  # max_path_length: 40000


# dataset:
#   _target_: diffuser.datasets.WaybabaMaze2dDataset
#   _partial_: true
#   env: "maze2d-openlarge-v0"
#   # only_start_end_episode: true
#   # custom_ds_path: ${paths.data_dir}/models/diffuser/maze2d-large-waybaba.hdf5
#   custom_ds_path: null
#   horizon: ${global_cfg.horizon}
#   normalizer: "LimitsNormalizer"
#   preprocess_fns: ["maze2d_set_terminals"] # why need this?
#   # preprocess_fns: []
#   use_padding: false
#   max_path_length: 40000

render:
  _target_: diffuser.utils.Maze2dRenderer
  _partial_: true


net:
  _target_: diffuser.models.TemporalUnet
  _partial_: true
  horizon: ${global_cfg.horizon}
  dim_mults: [1, 4, 8]
  attention: false

model:
  _target_: diffuser.models.GaussianDiffusion
  _partial_: true
  horizon: ${global_cfg.horizon}
  n_timesteps: 256 # n_diffusion_steps in source code
  loss_type: "l2"
  clip_denoised: true # ! ?
  predict_epsilon: false
  action_weight: 1 # ! only affect first step action
  loss_weights: null # 
  loss_discount: 1 # may need less weight for future steps
  ignore_action: true # if true, only apply loss on observations
  # TODO add config for large0maze 2 in py file



trainer:
  _target_: diffuser.utils.Trainer
  task: train_diffuser
  _partial_: true
  train_batch_size: 32
  train_lr: 2e-4
  gradient_accumulate_every: 2
  ema_decay: 0.995
  sample_freq: 100000 # ! for x step, render generated samples, turn to 1e5 for cloud based!
  save_freq: 100000 # for x step, save model
  label_freq: 10000 # not important, just use for name. e.g. 12234 -> 12000
  save_parallel: false
  results_folder: ${output_dir} # TODO
  bucket: null # TODO ? what
  n_reference: 25 # TODO ? what
  n_render_samples: 4
  condition_noise: 0.02 # apply a small noise to condition during training to avoid overfitting




# common - for all tasks (task_name, tags, output_dir, device)
algorithm_name: "DefaultAlgName"
task_name: "RL_Diffuser"
tags: ["debug"]